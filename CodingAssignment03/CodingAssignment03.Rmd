---
title: "Coding Assignment 3"
author: "Team 6"
date: "Due: 2024-12-07 23:59"
output:
  pdf_document:
    toc: true
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
#Put any packages you need here
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(car)
library(gt)
library(gtsummary)
library(corrplot)
library(tinytex)
```

A Florida health insurance company wants to predict annual claims for individual clients. The company pulls a random sample of 100 customers. The owner wishes to charge an actuarially fair premium to ensure a normal rate of return. The owner collects all of their current customer’s health care expenses from the last year and compares them with what is known about each customer’s plan.

The data on the 100 customers in the sample is as follows:

-   Charges: Total medical expenses for a particular insurance plan (in dollars)
-   Age: Age of the primary beneficiary
-   BMI: Primary beneficiary’s body mass index (kg/m2)
-   Female: Primary beneficiary’s birth sex (0 = Male, 1 = Female)
-   Children: Number of children covered by health insurance plan (includes other dependents as well)
-   Smoker: Indicator if primary beneficiary is a smoker (0 = non-smoker, 1 = smoker)
-   Cities: Dummy variables for each city with the default being Sanford

Answer the following questions using complete sentences and attach all output, plots, etc. within this report.

```{r dataset}
# Bring in the dataset here.

Claims <- read_xlsx("/Users/megibejleri/Documents/GitHub/Insurance_Data_Group6.xlsx")

gt(head(Claims))
```

## Question 1

Randomly select 30 observations from the sample and exclude from all modeling. Provide the summary statistics (min, max, std, mean, median) of the quantitative variables for the 70 observations.

```{r q1}

set.seed(123456)
index <- sample(seq_len(nrow(Claims)), size = 30)

train <- Claims[-index,]
test <- Claims[index,]

quant_train <- train %>%
  select(Charges, Age, BMI)

quant_train %>%
tbl_summary(statistic = list(all_continuous() ~ c("{mean} ({sd})",
"{median} ({p25}, {p75})",
"{min}, {max}"),
all_categorical() ~ "{n} / {N} ({p}%)"),
type = all_continuous() ~ "continuous2"
)

```

## Question 2

Provide the correlation between all quantitative variables

```{r q2}

corrplot(cor(quant_train))
         
```
>>>>>>> 9cd2ed9f1cb343e93601a9775429941a9d9d14cd

corrplot(cor(quant_train))
  
```

## Question 3

Run a regression that includes all independent variables in the data table. Does the model above violate any of the Gauss-Markov assumptions? If so, what are they and what is the solution for correcting?

<<<<<<< HEAD
```{r q3 fig.height= 8, fig.width=8}

original_model <- lm(Charges ~. , data = train)
summary(original_model)

scatterplotMatrix(train[,1:3])

par(mfrow=c(2,2))
plot(original_model)

#
```
The first plot titled “Residuals vs. Fitted,” shows a non-linear relationship, thus violates a classical assumption. 

The second plot titled “Normal Q-Q” would show a 45-degree line upwards. Since that is not the case here
the assumption of a normally distributed dependent variable for a fixed set of predictors is violated.

The third plot titled “Scale-Location” does not show some random points around a horizontal line rather has an upward slope indicating heteroscedasticity.

Applying a transformation to the dependent variable Charges (e.g., logarithmic, square root) would solve these issues.
=======
```{r q3}
#
```
>>>>>>> 9cd2ed9f1cb343e93601a9775429941a9d9d14cd


## Question 4

Implement the solutions from question 3, such as data transformation, along with any other changes you wish. Use the sample data and run a new regression. How have the fit measures changed? How have the signs and significance of the coefficients changed?

```{r q4}

#transform dataset - natural log & squared

#Charges
train$lnCharges <- log(train$Charges)
train$ChargesSquared <- train$Charges^2

#BMI
train$lnBMI <- log(train$BMI)
train$BMISquared <- train$BMI^2

#Age
train$lnAge <- log(train$Age)
train$AgeSquared <- train$Age^2

#Plot Changes - Before and After
#Charges
par(mfrow=c(1,3))
hist(train$Charges) #before
hist(train$lnCharges) #after ln
hist(train$ChargesSquared) #after squared

#BMI
par(mfrow=c(1,3))
hist(train$BMI) #before
hist(train$lnBMI) #after ln
hist(train$BMISquared) #after squared

#Age
par(mfrow=c(1,3))
hist(train$Age) #before
hist(train$lnAge) #after ln
hist(train$AgeSquared) #after squared

scatterplotMatrix(train[,c(1,2,3)]) # grabbing original
scatterplotMatrix(train[,c(10,11,14)]) # grabbing ln

model_log <- lm(lnCharges ~., data = train[,c(4:9,10,12,14)]) #pulling only columns I want
summary(model_log)

model_quad <- lm(ChargesSquared ~., data = train[,c(4:9,11,13,15)]) #pulling only columns I want
summary(model_quad)

par(mfrow=c(2,2))
plot(model_log)



```

## Question 5

Use the 30 withheld observations and calculate the performance measures for your best two models. Which is the better model? (remember that "better" depends on whether your outlook is short or long run)

```{r q5}
test$lnCharges <- log(test$Charges)
test$lnBMI <- log(test$BMI)
test$lnAge <- log(test$Age)

test$original_model_pred <- predict(original_model, newdata = test)
test$model_log_pred <- predict(model_log,newdata = test) %>% exp()
#test$model_quad_pred <- predict(model_quad,newdata = test) %>% exp()

# Finding the error
test$error_om <- test$original_model_pred - test$Charges
test$error_log <- test$model_log_pred - test$Charges
#test$error_quad <- test$model_quad_pred - test$Charges
#

#Bias
mean(test$error_om)
mean(test$error_log)

#MAE
mae <- function(error_vector){
error_vector %>%
abs() %>%
mean()
}

mae(test$error_om)
mae(test$error_log)

#RMSE
rmse <- function(error_vector){
error_vector^2 %>%
mean() %>%
sqrt()
}

rmse(test$error_om)
rmse(test$error_log)

#MAPE
mape <- function(error_vector, actual_vector){
(error_vector/actual_vector) %>%
abs() %>%
mean()
}

mape(test$error_om, test$Charges)
mape(test$error_log, test$Charges)

```
The Original Model is better for short-term prediction because it has a smaller bias, lower MAE, and lower RMSE, suggesting it will provide more accurate and reliable predictions for individual or immediate use.

The Log Model is better for long-term prediction due to its significantly lower MAPE. This suggests it will likely provide more accurate predictions relative to the actual values in terms of percentage error, which is important for long-term forecasts.


## Question 6

Provide interpretations of the coefficients, do the signs make sense? Perform marginal change analysis (thing 2) on the independent variables.

```{r q6}
#
```

## Question 7

An eager insurance representative comes back with five potential clients. Using the better of the two models selected above, provide the prediction intervals for the five potential clients using the information provided by the insurance rep.

| Customer | Age | BMI | Female | Children | Smoker | City           |
|----------|-----|-----|--------|----------|--------|----------------|
| 1        | 60  | 22  | 1      | 0        | 0      | Oviedo         |
| 2        | 40  | 30  | 0      | 1        | 0      | Sanford        |
| 3        | 25  | 25  | 0      | 0        | 1      | Winter Park    |
| 4        | 33  | 35  | 1      | 2        | 0      | Winter Springs |
| 5        | 45  | 27  | 1      | 3        | 0      | Oviedo         |

<<<<<<< HEAD
```{r}
=======

```{r q7}
>>>>>>> 9cd2ed9f1cb343e93601a9775429941a9d9d14cd
#
```

## Question 8

The owner notices that some of the predictions are wider than others, explain why.

<<<<<<< HEAD
## Question 9

Are there any prediction problems that occur with the five potential clients? If so, explain.
=======
```{r q8}
#
```

## Question 9 

Are there any prediction problems that occur with the five potential clients? If so, explain.

```{r q9}
#
```
## Question 9 

Are there any prediction problems that occur with the five potential clients? If so, explain.

```{r q9}
#
```

>>>>>>> 9cd2ed9f1cb343e93601a9775429941a9d9d14cd
