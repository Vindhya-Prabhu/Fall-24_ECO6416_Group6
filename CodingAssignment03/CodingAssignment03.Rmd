---
title: "Coding Assignment 3"
author: "Team 6"
date: "Due: 2024-12-07 23:59"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
#Put any packages you need here
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(car)
library(gt)
library(gtsummary)
library(corrplot)
library(tinytex)
```

A Florida health insurance company wants to predict annual claims for individual clients. The company pulls a random sample of 100 customers. The owner wishes to charge an actuarially fair premium to ensure a normal rate of return. The owner collects all of their current customer’s health care expenses from the last year and compares them with what is known about each customer’s plan.

The data on the 100 customers in the sample is as follows:

-   Charges: Total medical expenses for a particular insurance plan (in dollars)
-   Age: Age of the primary beneficiary
-   BMI: Primary beneficiary’s body mass index (kg/m2)
-   Female: Primary beneficiary’s birth sex (0 = Male, 1 = Female)
-   Children: Number of children covered by health insurance plan (includes other dependents as well)
-   Smoker: Indicator if primary beneficiary is a smoker (0 = non-smoker, 1 = smoker)
-   Cities: Dummy variables for each city with the default being Sanford

Answer the following questions using complete sentences and attach all output, plots, etc. within this report.

```{r dataset}
# Bring in the dataset here.

Claims <- read.csv("C:\\UserData\\z002r7kt\\OneDrive - Siemens AG\\Reference Documents\\Learnings\\Fall-24_ECO6416_Group6\\Insurance_Data_Group6.csv")


gt(head(Claims))
```

## Question 1

Randomly select 30 observations from the sample and exclude from all modeling. Provide the summary statistics (min, max, std, mean, median) of the quantitative variables for the 70 observations.

```{r q1}

set.seed(123456)
index <- sample(seq_len(nrow(Claims)), size = 30)

train <- Claims[-index,]
test <- Claims[index,]

quant_train <- train %>%
  select(Charges, Age, BMI, Children)

quant_train %>%
tbl_summary(statistic = list(all_continuous() ~ c("{mean} ({sd})",
"{median} ({p25}, {p75})",
"{min}, {max}"),
all_categorical() ~ "{n} / {N} ({p}%)"),
type = all_continuous() ~ "continuous2"

```

*****

## Question 2

Provide the correlation between all quantitative variables

```{r q2}

corrplot(cor(quant_train))

```

The correlation matrix shows weak positive relationships between **Charges** and both **Age 0.17** and **BMI 0.26**, while **Charges** has no meaningful correlation with **Children -0.002**. Among the predictors, correlations are generally negligible, with the strongest being between **Age** and **Children 0.23**. There is no indication of strong multicollinearity.

*****

## Question 3

Run a regression that includes all independent variables in the data table. Does the model above violate any of the Gauss-Markov assumptions? If so, what are they and what is the solution for correcting?

```{r q3}

original_model <- lm(Charges ~. , data = train[,c(1:9)])
summary(original_model)

scatterplotMatrix(train[,1:3])

par(mfrow=c(2,2))
plot(original_model)
```

The first plot titled “Residuals vs. Fitted,” shows a non-linear relationship, thus violates a classical assumption. 

The second plot titled “Normal Q-Q” would show a 45-degree line upwards. Since that is not the case here
the assumption of a normally distributed dependent variable for a fixed set of predictors is violated.

The third plot titled “Scale-Location” does not show some random points around a horizontal line rather has an upward slope indicating heteroscedasticity.

Applying a transformation to the dependent variable Charges (e.g., logarithmic, squared) would solve these issues.

*****

## Question 4

Implement the solutions from question 3, such as data transformation, along with any other changes you wish. Use the sample data and run a new regression. How have the fit measures changed? How have the signs and significance of the coefficients changed?

```{r q4}

#transform dataset - natural log & squared

#Charges
train$lnCharges <- log(train$Charges)
train$ChargesSquared <- train$Charges^2

#BMI
train$lnBMI <- log(train$BMI)
train$BMISquared <- train$BMI^2

#Age
train$lnAge <- log(train$Age)
train$AgeSquared <- train$Age^2


#Plot Changes - Before and After
#Charges
par(mfrow=c(1,3))
hist(train$Charges) #before
hist(train$lnCharges) #after ln
hist(train$ChargesSquared) #after squared

#BMI
par(mfrow=c(1,3))
hist(train$BMI) #before
hist(train$lnBMI) #after ln
hist(train$BMISquared) #after squared

#Age
par(mfrow=c(1,3))
hist(train$Age) #before
hist(train$lnAge) #after ln
hist(train$AgeSquared) #after squared


scatterplotMatrix(train[,c(1,2,3,5)]) # grabbing original
scatterplotMatrix(train[,c(10,12,14)]) # grabbing ln
scatterplotMatrix(train[,c(11,13,15)]) # grabbing squared

model_log <- lm(lnCharges ~., data = train[,c(4:9,10,12,14)]) #pulling new ln columns
summary(model_log)

par(mfrow=c(2,2))
plot(model_log)

model_quad <- lm(ChargesSquared ~., data = train[,c(4:9,11,13,15)]) #pulling new squared columns
summary(model_quad)

par(mfrow=c(2,2))
plot(model_quad)

```

Comparison of 3 models original_model (not transformed), model_log (log-transformed), and model_quad (squared) revealed that: 

The first model **original_model** showed moderate fit **0.851** and a higher Residual Standard Error **5211** compared to the second model. With this model coefficients Age, BMI and Smoker test significant and show a direct relationship with the dependent variable. Other independent variables in the data set did not test significant and show an inverse relationship with the dependent variable.

The second model **model_log** provided the best fit, with the highest Adjusted R-squared **0.856** and the lowest Residual Standard Error **0.387**. With this model coefficients Smoker, lnAge and lnBMI (although reduced) continue to test significant, additionally WinterPark test significant. Signs of the independent variables Female and Children have flipped while others have remained unchanged.

Third model **model_quad** shows non-linear relationships for BMI and Age, its weaker Adjusted R-squared **0.749** and high Residual Standard Error **309000000** indicated poor overall performance. With this model coefficients Smoker, BMISquared,and AgeSquared (although reduced) continues to test significant. Signs of the independent variables WinterSprings and Oviedo have flipped while others have remained unchanged.

Overall the second model **model_log** stood out as the most accurate.

*****

## Question 5

Use the 30 withheld observations and calculate the performance measures for your best two models. Which is the better model? (remember that "better" depends on whether your outlook is short or long run)

```{r q5}

test$lnCharges <- log(test$Charges)
test$lnBMI <- log(test$BMI)
test$lnAge <- log(test$Age)

test$original_model_pred <- predict(original_model, newdata = test)
test$model_log_pred <- predict(model_log,newdata = test) %>% exp()


# Finding the error
test$error_om <- test$original_model_pred - test$Charges
test$error_log <- test$model_log_pred - test$Charges

#

#Bias
mean(test$error_om)
mean(test$error_log)

#MAE
mae <- function(error_vector){
error_vector %>%
abs() %>%
mean()
}

mae(test$error_om)
mae(test$error_log)

#RMSE
rmse <- function(error_vector){
error_vector^2 %>%
mean() %>%
sqrt()
}

rmse(test$error_om)
rmse(test$error_log)

#MAPE
mape <- function(error_vector, actual_vector){
(error_vector/actual_vector) %>%
abs() %>%
mean()
}

mape(test$error_om, test$Charges)
mape(test$error_log, test$Charges)

```

The first model *original_model* is better for short-term prediction because it has a smaller bias, lower MAE, and lower RMSE, suggesting it will provide more accurate and reliable predictions for individual or immediate use.

The second model *model_log* is better for long-term prediction due to its significantly lower MAPE. This suggests it will likely provide more accurate predictions relative to the actual values in terms of percentage error, which is important for long-term forecasts.

*****

## Question 6

Provide interpretations of the coefficients, do the signs make sense? Perform marginal change analysis (thing 2) on the independent variables.

```{r q6}

model_log$coefficients["Female"]
model_log$coefficients["Children"]
model_log$coefficients["Smoker"]
model_log$coefficients["WinterSprings"]
model_log$coefficients["WinterPark"]
model_log$coefficients["Oviedo"]
model_log$coefficients["lnBMI"]
model_log$coefficients["lnAge"]

```
Interpertation of the coefficients: 

Being female leads to a 4.3% increase in expected charges compared to males, holding all other factors constant. Each additional child is associated with a 1.16% increase in expected charges holding all else constant. The sign makes sense since additional dependents could slightly increase healthcare usage. Being a smoker is associated with a 187.1% increase in expected charges compared to non-smokers. This large positive value makes sense since smoking is a major driver of healthcare costs due to associated risks. Living in Winter Springs you would expect a 20.2% decrease in charges. This sign makes sense if costs are geographically lower in Winter Springs. Living in Winter Park you would expectt a 28.4% decrease in charges. This sign makes sense if costs are geographically lower in Winter Park. Living in Oviedo you would expectt a 5.84% decrease in charges. Again, this sign makes sense if costs are geographically lower in Oviedo. As for BMI, a 1% increase in BMI is associated with a 0.7% increase in expected charges, holding other factors constant. This positive value makes sense, as higher BMI often correlates with increased health risks and costs. Finally, for Age, a 1% increase in Age is associated with a 1.3% increase in expected charges, holding other factors constant.This positive value makes sense as older individuals typically incur higher healthcare costs.

*****
## Question 7

An eager insurance representative comes back with five potential clients. Using the better of the two models selected above, provide the prediction intervals for the five potential clients using the information provided by the insurance rep.

| Customer | Age | BMI | Female | Children | Smoker | City           |
|----------|-----|-----|--------|----------|--------|----------------|
| 1        | 60  | 22  | 1      | 0        | 0      | Oviedo         |
| 2        | 40  | 30  | 0      | 1        | 0      | Sanford        |
| 3        | 25  | 25  | 0      | 0        | 1      | Winter Park    |
| 4        | 33  | 35  | 1      | 2        | 0      | Winter Springs |
| 5        | 45  | 27  | 1      | 3        | 0      | Oviedo         |



```{r q7}

new_clients <- data.frame(
  Age = c(60, 40, 25, 33, 45),
  BMI = c(22, 30, 25, 35, 27),
  Female = c(1, 0, 0, 1, 1),
  Children = c(0, 1, 0, 2, 3),
  Smoker = c(0, 0, 1, 0, 0),
  WinterSprings = c(0, 0, 0, 1, 0),
  WinterPark = c(0, 0, 1, 0, 0),
  Oviedo = c(1, 0, 0, 0, 1),
  lnBMI = log(c(22, 30, 25, 35, 27)),
  lnAge = log(c(60, 40, 25, 33, 45)))
  
  predictions <- predict(
  model_log,
  newdata = new_clients,
  interval = "prediction"
  )
  
print(predictions)

```

*****

## Question 8

The owner notices that some of the predictions are wider than others, explain why.

Based on the output of Question 7, intervals with wider ranges have either a smaller sample size or higher variability in the data compared to the narrower intervals. For example, we have High Age and low BMI combinations but in a smaller sample as well as High Children and Ovideo. Smoker status increases variability and Winter Park is underrepresented. Also, High BMI and Winter Springs are underrepresented.

*****

## Question 9

Are there any prediction problems that occur with the five potential clients? If so, explain.

Yes. Since the training data indicates a strong correlation between ln(BMI) and ln(Age), the predictions for clients with unusual combinations of these values (ex: Customer 1- old age with low BMI) could be less accurate. These clients with extreme values of predictors (ex: Age = 60, BMI = 22) may have larger prediction errors due to heteroscedasticity.

*****
