---
title: "Coding Assignment 3"
author: "Team 6"
date: "Due: 2024-12-07 23:59"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
#Put any packages you need here
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(car)
library(gt)
library(gtsummary)
library(corrplot)
library(tinytex)
```

A Florida health insurance company wants to predict annual claims for individual clients. The company pulls a random sample of 100 customers. The owner wishes to charge an actuarially fair premium to ensure a normal rate of return. The owner collects all of their current customer’s health care expenses from the last year and compares them with what is known about each customer’s plan.

The data on the 100 customers in the sample is as follows:

-   Charges: Total medical expenses for a particular insurance plan (in dollars)
-   Age: Age of the primary beneficiary
-   BMI: Primary beneficiary’s body mass index (kg/m2)
-   Female: Primary beneficiary’s birth sex (0 = Male, 1 = Female)
-   Children: Number of children covered by health insurance plan (includes other dependents as well)
-   Smoker: Indicator if primary beneficiary is a smoker (0 = non-smoker, 1 = smoker)
-   Cities: Dummy variables for each city with the default being Sanford

Answer the following questions using complete sentences and attach all output, plots, etc. within this report.

```{r dataset}
# Bring in the dataset here.

Claims <- read.csv("C:\\UserData\\z002r7kt\\OneDrive - Siemens AG\\Reference Documents\\Learnings\\Fall-24_ECO6416_Group6\\Insurance_Data_Group6.csv")


gt(head(Claims))
```

## Question 1

Randomly select 30 observations from the sample and exclude from all modeling. Provide the summary statistics (min, max, std, mean, median) of the quantitative variables for the 70 observations.

```{r q1}

set.seed(123456)
index <- sample(seq_len(nrow(Claims)), size = 30)

train <- Claims[-index,]
test <- Claims[index,]

quant_train <- train %>%
  select(Charges, Age, BMI)

quant_train %>%
tbl_summary(statistic = list(all_continuous() ~ c("{mean} ({sd})",
"{median} ({p25}, {p75})",
"{min}, {max}"),
all_categorical() ~ "{n} / {N} ({p}%)"),
type = all_continuous() ~ "continuous2"

```

*****

## Question 2

Provide the correlation between all quantitative variables

```{r q2}

corrplot(cor(quant_train))

```

The correlation analysis shows weak positive relationships between **Charges** and **Age 0.169** and **Charges** and **BMI 0.261**, while **Age** and **BMI** have almost no correlation **0.055**. These low values suggest minimal linear relationships between the variables.

*****

## Question 3

Run a regression that includes all independent variables in the data table. Does the model above violate any of the Gauss-Markov assumptions? If so, what are they and what is the solution for correcting?

```{r q3}

original_model <- lm(Charges ~. , data = train[,c(1:9)])
summary(original_model)

scatterplotMatrix(train[,1:3])

par(mfrow=c(2,2))
plot(original_model)
```

The first plot titled “Residuals vs. Fitted,” shows a non-linear relationship, thus violates a classical assumption. 

The second plot titled “Normal Q-Q” would show a 45-degree line upwards. Since that is not the case here
the assumption of a normally distributed dependent variable for a fixed set of predictors is violated.

The third plot titled “Scale-Location” does not show some random points around a horizontal line rather has an upward slope indicating heteroscedasticity.

Applying a transformation to the dependent variable Charges (e.g., logarithmic, squared) would solve these issues.

*****

## Question 4

Implement the solutions from question 3, such as data transformation, along with any other changes you wish. Use the sample data and run a new regression. How have the fit measures changed? How have the signs and significance of the coefficients changed?

```{r q4}

#transform dataset - natural log & squared

#Charges
train$lnCharges <- log(train$Charges)
train$ChargesSquared <- train$Charges^2

#BMI
train$lnBMI <- log(train$BMI)
train$BMISquared <- train$BMI^2

#Age
train$lnAge <- log(train$Age)
train$AgeSquared <- train$Age^2

#Plot Changes - Before and After
#Charges
par(mfrow=c(1,3))
hist(train$Charges) #before
hist(train$lnCharges) #after ln
hist(train$ChargesSquared) #after squared

#BMI
par(mfrow=c(1,3))
hist(train$BMI) #before
hist(train$lnBMI) #after ln
hist(train$BMISquared) #after squared

#Age
par(mfrow=c(1,3))
hist(train$Age) #before
hist(train$lnAge) #after ln
hist(train$AgeSquared) #after squared

scatterplotMatrix(train[,c(1,2,3)]) # grabbing original
scatterplotMatrix(train[,c(10,12,14)]) # grabbing ln
scatterplotMatrix(train[,c(11,13,15)]) # grabbing squared

model_log <- lm(lnCharges ~., data = train[,c(4:9,10,12,14)]) #pulling new ln columns
summary(model_log)

par(mfrow=c(2,2))
plot(model_log)

model_quad <- lm(ChargesSquared ~., data = train[,c(4:9,11,13,15)]) #pulling new squared columns
summary(model_quad)

par(mfrow=c(2,2))
plot(model_quad)

```

Comparison of 3 models original_model (not transformed), model_log (log-transformed), and model_quad (squared) revealed that: 

The first model **original_model** showed moderate fit **0.851** and a higher Residual Standard Error **5211** compared to the second model. With this model coefficients Age, BMI and Smoker test significant and show a direct relationship with the dependent variable. Other independent variables in the data set did not test significant and show an inverse relationship with the dependent variable.

The second model **model_log** provided the best fit, with the highest Adjusted R-squared **0.856** and the lowest Residual Standard Error **0.387**. With this model coefficients Smoker, lnAge and lnBMI (although reduced) continue to test significant, additionally WinterPark test significant. Signs of the independent variables Female and Children have flipped while others have remained unchanged.

Third model **model_quad** shows non-linear relationships for BMI and Age, its weaker Adjusted R-squared **0.749** and high Residual Standard Error **309000000** indicated poor overall performance. With this model coefficients Smoker, BMISquared,and AgeSquared (although reduced) continues to test significant. Signs of the independent variables WinterSprings and Oviedo have flipped while others have remained unchanged.

Overall the second model **model_log** stood out as the most accurate.

*****

## Question 5

Use the 30 withheld observations and calculate the performance measures for your best two models. Which is the better model? (remember that "better" depends on whether your outlook is short or long run)

```{r q5}

test$lnCharges <- log(test$Charges)
test$lnBMI <- log(test$BMI)
test$lnAge <- log(test$Age)

test$original_model_pred <- predict(original_model, newdata = test)
test$model_log_pred <- predict(model_log,newdata = test) %>% exp()


# Finding the error
test$error_om <- test$original_model_pred - test$Charges
test$error_log <- test$model_log_pred - test$Charges

#

#Bias
mean(test$error_om)
mean(test$error_log)

#MAE
mae <- function(error_vector){
error_vector %>%
abs() %>%
mean()
}

mae(test$error_om)
mae(test$error_log)

#RMSE
rmse <- function(error_vector){
error_vector^2 %>%
mean() %>%
sqrt()
}

rmse(test$error_om)
rmse(test$error_log)

#MAPE
mape <- function(error_vector, actual_vector){
(error_vector/actual_vector) %>%
abs() %>%
mean()
}

mape(test$error_om, test$Charges)
mape(test$error_log, test$Charges)

```

The first model *original_model* is better for short-term prediction because it has a smaller bias, lower MAE, and lower RMSE, suggesting it will provide more accurate and reliable predictions for individual or immediate use.

The second model *model_log* is better for long-term prediction due to its significantly lower MAPE. This suggests it will likely provide more accurate predictions relative to the actual values in terms of percentage error, which is important for long-term forecasts.

*****

## Question 6

Provide interpretations of the coefficients, do the signs make sense? Perform marginal change analysis (thing 2) on the independent variables.

```{r q6}

model_log$coefficients["Female"]
model_log$coefficients["Children"]
model_log$coefficients["Smoker"]
model_log$coefficients["WinterSprings"]
model_log$coefficients["WinterPark"]
model_log$coefficients["Oviedo"]
model_log$coefficients["lnBMI"]
model_log$coefficients["lnAge"]

```

*****
## Question 7

An eager insurance representative comes back with five potential clients. Using the better of the two models selected above, provide the prediction intervals for the five potential clients using the information provided by the insurance rep.

| Customer | Age | BMI | Female | Children | Smoker | City           |
|----------|-----|-----|--------|----------|--------|----------------|
| 1        | 60  | 22  | 1      | 0        | 0      | Oviedo         |
| 2        | 40  | 30  | 0      | 1        | 0      | Sanford        |
| 3        | 25  | 25  | 0      | 0        | 1      | Winter Park    |
| 4        | 33  | 35  | 1      | 2        | 0      | Winter Springs |
| 5        | 45  | 27  | 1      | 3        | 0      | Oviedo         |



```{r q7}

new_clients <- data.frame(
  Age = c(60, 40, 25, 33, 45),
  BMI = c(22, 30, 25, 35, 27),
  Female = c(1, 0, 0, 1, 1),
  Children = c(0, 1, 0, 2, 3),
  Smoker = c(0, 0, 1, 0, 0),
  WinterSprings = c(0, 0, 0, 1, 0),
  WinterPark = c(0, 0, 1, 0, 0),
  Oviedo = c(1, 0, 0, 0, 1),
  lnBMI = log(c(22, 30, 25, 35, 27)),
  lnAge = log(c(60, 40, 25, 33, 45)))
  
  predictions <- predict(
  model_log,
  newdata = new_clients,
  interval = "prediction"
  )
  
print(predictions)

```

*****

## Question 8

The owner notices that some of the predictions are wider than others, explain why.

*****

## Question 9

Are there any prediction problems that occur with the five potential clients? If so, explain.

*****
